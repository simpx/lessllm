#!/usr/bin/env python3
"""
LessLLM æµ‹è¯•å®¢æˆ·ç«¯ç¤ºä¾‹
"""

import asyncio
import httpx
import json
import os
from datetime import datetime


async def test_lessllm_proxy():
    """æµ‹è¯•LessLLMä»£ç†æœåŠ¡"""
    
    # LessLLMä»£ç†æœåŠ¡åœ°å€
    base_url = "http://localhost:8000"
    
    async with httpx.AsyncClient() as client:
        
        # 1. å¥åº·æ£€æŸ¥
        print("ğŸ” Testing health check...")
        try:
            response = await client.get(f"{base_url}/health")
            print(f"âœ“ Health check: {response.status_code}")
            print(f"  Response: {response.json()}")
        except Exception as e:
            print(f"âœ— Health check failed: {e}")
            return
        
        # 2. åˆ—å‡ºå¯ç”¨æ¨¡å‹
        print("\nğŸ¤– Testing models endpoint...")
        try:
            response = await client.get(f"{base_url}/v1/models")
            print(f"âœ“ Models: {response.status_code}")
            models = response.json()
            print(f"  Available models: {len(models.get('data', []))}")
            for model in models.get('data', [])[:3]:
                print(f"    - {model['id']}")
        except Exception as e:
            print(f"âœ— Models endpoint failed: {e}")
        
        # 3. æµ‹è¯•èŠå¤©å®Œæˆ (éæµå¼)
        print("\nğŸ’¬ Testing chat completions (non-streaming)...")
        try:
            chat_request = {
                "model": "gpt-3.5-turbo",
                "messages": [
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": "Hello! Please respond with a short greeting."}
                ],
                "max_tokens": 50,
                "temperature": 0.7
            }
            
            response = await client.post(
                f"{base_url}/v1/chat/completions",
                json=chat_request,
                timeout=30.0
            )
            
            print(f"âœ“ Chat completion: {response.status_code}")
            if response.status_code == 200:
                result = response.json()
                if "choices" in result and result["choices"]:
                    content = result["choices"][0]["message"]["content"]
                    print(f"  Response: {content[:100]}...")
                    
                if "usage" in result:
                    usage = result["usage"]
                    print(f"  Tokens used: {usage.get('total_tokens', 'N/A')}")
            else:
                print(f"  Error: {response.text}")
                
        except Exception as e:
            print(f"âœ— Chat completion failed: {e}")
        
        # 4. æµ‹è¯•æµå¼èŠå¤©å®Œæˆ
        print("\nğŸŒŠ Testing streaming chat completions...")
        try:
            chat_request = {
                "model": "gpt-3.5-turbo",
                "messages": [
                    {"role": "user", "content": "Count from 1 to 5, each number on a new line."}
                ],
                "max_tokens": 30,
                "stream": True
            }
            
            chunks_received = 0
            content_parts = []
            
            async with client.stream(
                "POST",
                f"{base_url}/v1/chat/completions",
                json=chat_request,
                timeout=30.0
            ) as response:
                
                print(f"âœ“ Streaming started: {response.status_code}")
                
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data = line[6:]
                        if data == "[DONE]":
                            break
                        
                        try:
                            chunk = json.loads(data)
                            chunks_received += 1
                            
                            if ("choices" in chunk and 
                                chunk["choices"] and 
                                "delta" in chunk["choices"][0] and
                                "content" in chunk["choices"][0]["delta"]):
                                content = chunk["choices"][0]["delta"]["content"]
                                content_parts.append(content)
                                
                        except json.JSONDecodeError:
                            continue
            
            print(f"  Chunks received: {chunks_received}")
            print(f"  Content: {''.join(content_parts)[:100]}...")
            
        except Exception as e:
            print(f"âœ— Streaming chat completion failed: {e}")
        
        # 5. è·å–ç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“Š Testing stats endpoint...")
        try:
            response = await client.get(f"{base_url}/lessllm/stats")
            if response.status_code == 200:
                stats = response.json()
                print(f"âœ“ Stats retrieved")
                if "database" in stats:
                    db_stats = stats["database"]
                    print(f"  Total records: {db_stats.get('total_records', 'N/A')}")
                    print(f"  DB size: {db_stats.get('db_size_mb', 0):.2f} MB")
            else:
                print(f"  Stats not available: {response.status_code}")
        except Exception as e:
            print(f"âœ— Stats endpoint failed: {e}")


def test_openai_compatibility():
    """æµ‹è¯•OpenAIå®¢æˆ·ç«¯å…¼å®¹æ€§"""
    print("\nğŸ”Œ Testing OpenAI client compatibility...")
    
    try:
        # è¿™é‡Œå¯ä»¥ä½¿ç”¨çœŸå®çš„OpenAIå®¢æˆ·ç«¯åº“æ¥æµ‹è¯•å…¼å®¹æ€§
        import openai
        
        # è®¾ç½®è‡ªå®šä¹‰base_urlæŒ‡å‘LessLLM
        client = openai.OpenAI(
            api_key="dummy-key",  # LessLLMä¼šä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„çœŸå®API key
            base_url="http://localhost:8000/v1"
        )
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "user", "content": "Say hello in one word"}
            ],
            max_tokens=5
        )
        
        print("âœ“ OpenAI client compatibility test passed")
        print(f"  Response: {response.choices[0].message.content}")
        
    except ImportError:
        print("  OpenAI library not installed, skipping compatibility test")
    except Exception as e:
        print(f"âœ— OpenAI client compatibility test failed: {e}")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ LessLLM Proxy Test Suite")
    print(f"Time: {datetime.now().isoformat()}")
    print("="*50)
    
    await test_lessllm_proxy()
    
    print("\n" + "="*50)
    test_openai_compatibility()
    
    print("\nâœ… Test suite completed!")
    print("\nNote: Make sure to:")
    print("1. Start LessLLM server: lessllm server --config examples/lessllm.yaml")
    print("2. Set your API keys as environment variables")
    print("3. Configure proxy settings if needed")


if __name__ == "__main__":
    asyncio.run(main())