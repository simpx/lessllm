#!/usr/bin/env python3
"""
æµ‹è¯•å¢å¼ºçš„åˆ†æåŠŸèƒ½ - ç”Ÿæˆç¤ºä¾‹æ•°æ®
"""

import sys
import os
import time
import random
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

from lessllm.logging.storage import LogStorage
from lessllm.logging.models import APICallLog, RawAPIData, EstimatedAnalysis, PerformanceAnalysis, CacheAnalysis

def generate_sample_data(storage: LogStorage, num_requests: int = 20):
    """ç”Ÿæˆç¤ºä¾‹æ•°æ®ç”¨äºæµ‹è¯•åˆ†æåŠŸèƒ½"""
    
    print(f"ç”Ÿæˆ {num_requests} ä¸ªç¤ºä¾‹è¯·æ±‚æ•°æ®...")
    
    providers = ["claude", "openai"]
    models = [
        "claude-3-5-sonnet-20241022",
        "claude-3-haiku-20240307",
        "gpt-4",
        "gpt-3.5-turbo"
    ]
    endpoints = ["messages", "chat/completions"]
    
    base_time = datetime.now() - timedelta(hours=24)
    
    for i in range(num_requests):
        provider = random.choice(providers)
        model = random.choice(models)
        endpoint = random.choice(endpoints)
        
        # æ¨¡æ‹ŸçœŸå®çš„tokenä½¿ç”¨æ¨¡å¼
        prompt_tokens = random.randint(10, 500)
        completion_tokens = random.randint(5, 200)
        total_tokens = prompt_tokens + completion_tokens
        
        # æ¨¡æ‹Ÿç¼“å­˜æ•°æ® (30%çš„è¯·æ±‚æœ‰ç¼“å­˜)
        has_cache = random.random() < 0.3
        cached_tokens = random.randint(0, prompt_tokens // 2) if has_cache else 0
        cache_hit_rate = cached_tokens / prompt_tokens if prompt_tokens > 0 and has_cache else 0
        
        # æ¨¡æ‹Ÿæ€§èƒ½æ•°æ®
        ttft_ms = random.randint(100, 1000)
        tpot_ms = random.uniform(10, 50)
        total_latency = ttft_ms + (completion_tokens * tpot_ms)
        tokens_per_second = completion_tokens / (total_latency / 1000) if total_latency > 0 else 0
        
        # æ¨¡æ‹Ÿæˆæœ¬ (ç®€åŒ–è®¡ç®—)
        cost_per_1k_input = 0.003 if "claude" in model else 0.0015
        cost_per_1k_output = 0.015 if "claude" in model else 0.002
        estimated_cost = (prompt_tokens * cost_per_1k_input + completion_tokens * cost_per_1k_output) / 1000
        
        # åˆ›å»ºè¯·æ±‚æ•°æ®
        request_data = {
            "model": model,
            "messages": [{"role": "user", "content": f"Test request {i+1}"}],
            "max_tokens": completion_tokens,
            "stream": random.choice([True, False])
        }
        
        # åˆ›å»ºå“åº”æ•°æ®
        response_data = {
            "type": "message" if endpoint == "messages" else "chat.completion",
            "content": [{"type": "text", "text": f"This is test response {i+1}"}],
            "usage": {
                "input_tokens": prompt_tokens,
                "output_tokens": completion_tokens,
                "total_tokens": total_tokens
            }
        }
        
        # åˆ›å»ºåŸå§‹æ•°æ®
        raw_data = RawAPIData(
            raw_request=request_data,
            raw_response=response_data,
            extracted_usage={
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": total_tokens
            },
            extracted_cache_info={
                "cached_tokens": cached_tokens,
                "cache_hit_rate": cache_hit_rate
            } if has_cache else None,
            # HTTPä¿¡æ¯
            request_headers={"content-type": "application/json", "user-agent": "test-client"},
            client_ip="127.0.0.1",
            user_agent="test-client/1.0",
            request_url=f"http://localhost:8000/v1/{endpoint}",
            request_method="POST",
            response_status_code=200,
            response_headers={"content-type": "application/json"}
        )
        
        # åˆ›å»ºæ€§èƒ½åˆ†æ
        performance_analysis = PerformanceAnalysis(
            ttft_ms=ttft_ms,
            tpot_ms=tpot_ms,
            total_latency_ms=int(total_latency),
            tokens_per_second=round(tokens_per_second, 2)
        )
        
        # åˆ›å»ºç¼“å­˜åˆ†æ
        cache_analysis = CacheAnalysis(
            estimated_cache_hit_rate=cache_hit_rate if has_cache else 0.0,
            estimated_cached_tokens=cached_tokens if has_cache else 0,
            estimated_fresh_tokens=prompt_tokens - cached_tokens if has_cache else prompt_tokens
        )
        
        # åˆ›å»ºä¼°ç®—åˆ†æ
        estimated_analysis = EstimatedAnalysis(
            estimated_performance=performance_analysis,
            estimated_cache=cache_analysis,
            estimated_cost_usd=estimated_cost
        )
        
        # åˆ›å»ºAPIè°ƒç”¨æ—¥å¿—
        log = APICallLog(
            request_id=f"test_req_{i+1:04d}_{int(time.time()*1000)}",
            provider=provider,
            model=model,
            endpoint=endpoint,
            raw_data=raw_data,
            estimated_analysis=estimated_analysis,
            success=random.random() > 0.05,  # 95%æˆåŠŸç‡
            error_message=None if random.random() > 0.05 else f"Test error {i+1}",
            timestamp=base_time + timedelta(minutes=i*30)
        )
        
        # å­˜å‚¨åˆ°æ•°æ®åº“
        storage.store_log(log)
        print(f"âœ… ç”Ÿæˆè¯·æ±‚ {i+1}/{num_requests}: {provider}/{model} - {prompt_tokens}+{completion_tokens}={total_tokens} tokens")
    
    print(f"\nğŸ‰ æˆåŠŸç”Ÿæˆ {num_requests} ä¸ªç¤ºä¾‹è¯·æ±‚!")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª LessLLM å¢å¼ºåˆ†æåŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    # åˆå§‹åŒ–å­˜å‚¨
    try:
        storage = LogStorage("./lessllm_logs.db")
        print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return
    
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    try:
        generate_sample_data(storage, num_requests=30)
    except Exception as e:
        print(f"âŒ æ•°æ®ç”Ÿæˆå¤±è´¥: {e}")
        return
    
    print("\nğŸ“Š æµ‹è¯•å»ºè®®:")
    print("1. å¯åŠ¨GUI: streamlit run gui/dashboard.py")
    print("2. æŸ¥çœ‹å…³é”®æŒ‡æ ‡éƒ¨åˆ†çš„æ–°ç»Ÿè®¡æ•°æ®")
    print("3. æ£€æŸ¥è¯·æ±‚åˆ—è¡¨ä¸­çš„æ–°åˆ—æ˜¾ç¤º")
    print("4. å°è¯•æ–°çš„SQLæŸ¥è¯¢æ¨¡æ¿")
    print("5. è§‚å¯Ÿæ•°æ®å¯è§†åŒ–å›¾è¡¨")
    
    print("\nğŸ” å¯æµ‹è¯•çš„æ–°åŠŸèƒ½:")
    print("- Tokenè¯¦ç»†ç»Ÿè®¡ (è¾“å…¥/è¾“å‡º/ç¼“å­˜)")
    print("- ç¼“å­˜æ•ˆç‡åˆ†æ")
    print("- æˆæœ¬æ•ˆç‡æ’è¡Œ")
    print("- æ€§èƒ½è¶‹åŠ¿å›¾è¡¨")
    print("- Providerå’Œæ¨¡å‹ä½¿ç”¨åˆ†å¸ƒ")

if __name__ == "__main__":
    main()