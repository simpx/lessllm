# LessLLM

> A lightweight LLM API proxy framework with network connectivity and usage analysis

**Do more with less code/gpu/mem**

## Overview

LessLLM is a lightweight Python framework that acts as a proxy for LLM APIs, designed to solve network connectivity issues and provide comprehensive usage analysis.

## Key Features

ğŸŒ **Network Connectivity**
- HTTP/HTTPS proxy support
- SOCKS4/5 proxy support
- Proxy authentication
- Connection pooling and timeout control

ğŸ“Š **Usage Analysis**
- Complete API request/response logging
- Performance metrics (TTFT/TPOT)
- Intelligent cache analysis
- Cost estimation and tracking
- DuckDB storage with SQL queries

ğŸ”Œ **API Compatibility**
- OpenAI API compatible
- Claude/Anthropic API support
- Streaming and non-streaming requests
- Multiple provider support

## Quick Start

### Installation

```bash
pip install -e .
```

### Initialize Configuration

```bash
lessllm init --output lessllm.yaml
```

### Start the Proxy Server

```bash
# Set your API keys
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-claude-key"

# Start server
lessllm server --config lessllm.yaml --port 8000
```

### Use with OpenAI Client

```python
import openai

# Point OpenAI client to LessLLM proxy
client = openai.OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy"  # LessLLM uses configured keys
)

response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

## Configuration

Create a `lessllm.yaml` configuration file:

```yaml
proxy:
  socks_proxy: "socks5://127.0.0.1:1080"  # Optional
  timeout: 30

providers:
  openai:
    api_key: "${OPENAI_API_KEY}"
  claude:
    api_key: "${ANTHROPIC_API_KEY}"

logging:
  enabled: true
  storage:
    type: "duckdb"
    db_path: "./lessllm_logs.db"

analysis:
  enable_cache_estimation: true
  enable_performance_tracking: true
```

## Project Status

**Current Implementation Status:**

âœ… Core Framework
- âœ… Configuration management system
- âœ… FastAPI proxy server
- âœ… Network proxy support (HTTP/SOCKS)
- âœ… Provider abstraction layer

âœ… API Support
- âœ… OpenAI provider implementation
- âœ… Claude provider implementation
- âœ… Streaming and non-streaming support

âœ… Logging & Analysis
- âœ… DuckDB storage system
- âœ… Performance tracking (TTFT/TPOT)
- âœ… Cache estimation algorithms
- âœ… Cost calculation utilities

âœ… Developer Tools
- âœ… CLI interface
- âœ… Example configurations
- âœ… Test client scripts

## Next Steps

ğŸ”„ **In Progress:**
- Unit tests and integration tests
- Documentation improvements
- Performance optimizations

ğŸ“‹ **Planned:**
- Web dashboard for usage analytics
- More LLM provider integrations
- Advanced caching strategies
- Batch request optimization

## License

MIT License