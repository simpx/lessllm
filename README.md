# LessLLM

> A lightweight LLM API proxy framework with network connectivity and usage analysis

**Do more with less code/gpu/mem**

## Overview

LessLLM is a lightweight Python framework that acts as a proxy for LLM APIs, designed to solve network connectivity issues and provide comprehensive usage analysis.

## Key Features

üåê **Network Connectivity**
- HTTP/HTTPS proxy support
- SOCKS4/5 proxy support
- Proxy authentication
- Connection pooling and timeout control

üìä **Usage Analysis**
- Complete API request/response logging
- Performance metrics (TTFT/TPOT)
- Intelligent cache analysis
- Cost estimation and tracking
- DuckDB storage with SQL queries

üîå **API Compatibility**
- OpenAI API compatible
- Claude/Anthropic API support
- Streaming and non-streaming requests
- Multiple provider support

## Quick Start

### Installation

```bash
pip install -e .
```

### Initialize Configuration

```bash
lessllm init --output lessllm.yaml
```

### Start the Proxy Server

```bash
# Set your API keys
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-claude-key"

# Start server
lessllm server --config lessllm.yaml --port 8000
```

### Start the Analytics Dashboard GUI

```bash
# Start GUI (requires optional GUI dependencies)
pip install -e .[gui]
lessllm gui --port 8501 --host localhost
```

### Use with OpenAI Client

```python
import openai

# Point OpenAI client to LessLLM proxy
client = openai.OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy"  # LessLLM uses configured keys
)

response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

## Configuration

Create a `lessllm.yaml` configuration file:

```yaml
proxy:
  socks_proxy: "socks5://127.0.0.1:1080"  # Optional
  timeout: 30

providers:
  openai:
    api_key: "${OPENAI_API_KEY}"
  claude:
    api_key: "${ANTHROPIC_API_KEY}"

logging:
  enabled: true
  storage:
    type: "duckdb"
    db_path: "./lessllm_logs.db"

analysis:
  enable_cache_estimation: true
  enable_performance_tracking: true
```

## Project Status

**Current Implementation Status:**

‚úÖ Core Framework
- ‚úÖ Configuration management system
- ‚úÖ FastAPI proxy server
- ‚úÖ Network proxy support (HTTP/SOCKS)
- ‚úÖ Provider abstraction layer

‚úÖ API Support
- ‚úÖ OpenAI provider implementation
- ‚úÖ Claude provider implementation
- ‚úÖ Streaming and non-streaming support

‚úÖ Logging & Analysis
- ‚úÖ DuckDB storage system
- ‚úÖ Performance tracking (TTFT/TPOT)
- ‚úÖ Cache estimation algorithms
- ‚úÖ Cost calculation utilities

‚úÖ Developer Tools
- ‚úÖ CLI interface
- ‚úÖ Example configurations
- ‚úÖ Test client scripts
- ‚úÖ Analytics dashboard GUI

## Next Steps

üîÑ **In Progress:**
- Unit tests and integration tests
- Documentation improvements
- Performance optimizations

üìã **Planned:**
- More LLM provider integrations
- Advanced caching strategies
- Batch request optimization

## License

MIT License